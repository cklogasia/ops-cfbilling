{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfada7b0-ea66-414c-b7a7-83ea3fe2ba9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:13.987874Z",
     "iopub.status.busy": "2025-08-31T12:26:13.987874Z",
     "iopub.status.idle": "2025-08-31T12:26:18.918997Z",
     "shell.execute_reply": "2025-08-31T12:26:18.918997Z"
    },
    "papermill": {
     "duration": 4.957447,
     "end_time": "2025-08-31T12:26:18.921612",
     "exception": false,
     "start_time": "2025-08-31T12:26:13.964165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inbound Data Loaded Successfully!\n",
      "Shape: (1830, 28) (rows, columns)\n"
     ]
    }
   ],
   "source": [
    "# === SUPPRESS OPENPYXL WARNINGS ===\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\n",
    "\n",
    "# === STEP 1: LOAD THE NEW OUTBOUND DATA ===\n",
    "# === INITIAL SETUP: RUN THIS CELL FIRST ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to your new Inbound file\n",
    "inbound_file_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Inbound Report Data.xlsx\" # <<< CHANGE THIS TO YOUR ACTUAL FILE PATH\n",
    "\n",
    "# Read the new file into a DataFrame called 'inbound_df'\n",
    "inbound_df = pd.read_excel(inbound_file_path)\n",
    "\n",
    "# Let's see what we're working with\n",
    "print(\"âœ… Inbound Data Loaded Successfully!\")\n",
    "print(f\"Shape: {inbound_df.shape} (rows, columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94753348-c225-4102-b45c-985bf4c57045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:19.091370Z",
     "iopub.status.busy": "2025-08-31T12:26:19.085608Z",
     "iopub.status.idle": "2025-08-31T12:26:19.102028Z",
     "shell.execute_reply": "2025-08-31T12:26:19.102028Z"
    },
    "papermill": {
     "duration": 0.04277,
     "end_time": "2025-08-31T12:26:19.102028",
     "exception": false,
     "start_time": "2025-08-31T12:26:19.059258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add new columns with default values\n",
    "inbound_df['01_Inbound Type'] = ''\n",
    "inbound_df['Inbound Qty (UOM)'] = 0\n",
    "inbound_df['Inbound Qty > 8'] = ''\n",
    "inbound_df['1H / 2H(Storage)'] = ''\n",
    "inbound_df['Unstuffing Type'] = ''\n",
    "inbound_df['Unstuffing'] = ''\n",
    "inbound_df['SFA Booking'] = ''\n",
    "inbound_df['SFA Status'] = ''\n",
    "inbound_df['Sorting'] = ''\n",
    "inbound_df['Date_PalletID_Location'] = ''\n",
    "inbound_df['Storage Type'] = ''\n",
    "inbound_df['Remarks'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b8680f-96a2-40f0-ba64-1676c73a5651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:19.147801Z",
     "iopub.status.busy": "2025-08-31T12:26:19.147801Z",
     "iopub.status.idle": "2025-08-31T12:26:19.177597Z",
     "shell.execute_reply": "2025-08-31T12:26:19.177597Z"
    },
    "papermill": {
     "duration": 0.054555,
     "end_time": "2025-08-31T12:26:19.180301",
     "exception": false,
     "start_time": "2025-08-31T12:26:19.125746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excel formula applied! Reference: PO No, Result: 01_Inbound Type"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  PO No 01_Inbound Type\n",
      "0   PO00058097-01082025       Receiving\n",
      "1   PO00058097-01082025       Receiving\n",
      "2   PO00058097-01082025       Receiving\n",
      "3   PO00058097-01082025       Receiving\n",
      "4   PO00058097-01082025       Receiving\n",
      "5   PO00058097-01082025       Receiving\n",
      "6           CGMU5171930          Import\n",
      "7           CGMU5171930          Import\n",
      "8           CGMU5171930          Import\n",
      "9           CGMU5171930          Import\n",
      "10          CGMU5171930          Import\n",
      "11          CGMU5171930          Import\n",
      "12          CGMU5171930          Import\n",
      "13          CGMU5171930          Import\n",
      "14          CGMU5171930          Import\n"
     ]
    }
   ],
   "source": [
    "# === APPLY EXCEL FORMULA: REFERENCE D2, RESULT IN AC2 ===\n",
    "\n",
    "# First, create a clean series by trimming whitespace from the D2 column (the reference)\n",
    "clean_series = inbound_df['PO No'].str.strip()  # TRIM values from D2 column\n",
    "\n",
    "# Normalize: trim spaces + lowercase for case-insensitive checks\n",
    "clean_series_norm = clean_series.str.strip().str.lower()\n",
    "\n",
    "conditions = [\n",
    "    clean_series_norm.str.startswith('po'),\n",
    "    clean_series_norm.str.startswith('to'),\n",
    "    clean_series_norm.str.startswith('sro'),\n",
    "    clean_series_norm.str.startswith('for '),\n",
    "    clean_series_norm.str.startswith('ret'),\n",
    "    clean_series_norm.str.startswith('order'),\n",
    "    clean_series_norm == ''\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    'Receiving',\n",
    "    'Plant Transfer', \n",
    "    'Return',\n",
    "    'Return',\n",
    "    'Return',\n",
    "    'Return',\n",
    "    '-'  # This handles the empty string case\n",
    "]\n",
    "\n",
    "# Apply the conditions and put the result in AC2 column\n",
    "inbound_df['01_Inbound Type'] = np.select(conditions, choices, default='Import')\n",
    "\n",
    "print(\"âœ… Excel formula applied! Reference: PO No, Result: 01_Inbound Type\")\n",
    "print(inbound_df[['PO No', '01_Inbound Type']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c51a53e-1e74-4716-aa5e-00709423ba81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:19.227162Z",
     "iopub.status.busy": "2025-08-31T12:26:19.227162Z",
     "iopub.status.idle": "2025-08-31T12:26:19.251520Z",
     "shell.execute_reply": "2025-08-31T12:26:19.250508Z"
    },
    "papermill": {
     "duration": 0.056363,
     "end_time": "2025-08-31T12:26:19.251520",
     "exception": false,
     "start_time": "2025-08-31T12:26:19.195157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 1 Complete: 'Inbound Qty (UOM)' updated for UOM = 'CTN'\n",
      "âœ… Step 2 Complete: 'Inbound Qty (UOM)' updated based on Article rules.\n",
      "\n",
      "ðŸ” Final Check - Sample of relevant columns:\n",
      "          Article  UOM  Qty Received  Inbound Qty (UOM)\n",
      "0   ST-C230000588  CTN          65.0               65.0\n",
      "1   ST-C230000588  CTN          65.0               65.0\n",
      "2   ST-C230000588  CTN          65.0               65.0\n",
      "3   ST-C230000588  CTN          65.0               65.0\n",
      "4   ST-C230000588  CTN          65.0               65.0\n",
      "5   ST-C230000588  CTN          25.0               25.0\n",
      "6   ST-C230000406  CTN          42.0               42.0\n",
      "7   ST-C230000406  CTN          42.0               42.0\n",
      "8   ST-C230000406  CTN          42.0               42.0\n",
      "9   ST-C230000406  CTN          42.0               42.0\n",
      "10  ST-C230000406  CTN          42.0               42.0\n",
      "11  ST-C230000406  CTN          42.0               42.0\n",
      "12  ST-C230000406  CTN          42.0               42.0\n",
      "13  ST-C230000406  CTN          42.0               42.0\n",
      "14  ST-C230000406  CTN          42.0               42.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === PART 1: Update 'Inbound Qty (UOM)' for UOM == 'CTN' ===\n",
    "# This sets the value for rows where UOM is 'CTN'\n",
    "inbound_df.loc[inbound_df['UOM'] == 'CTN', 'Inbound Qty (UOM)'] = inbound_df.loc[inbound_df['UOM'] == 'CTN', 'Qty Received']\n",
    "print(\"âœ… Step 1 Complete: 'Inbound Qty (UOM)' updated for UOM = 'CTN'\")\n",
    "\n",
    "# === PART 2: OVERRIDE 'Inbound Qty (UOM)' for specific Articles ===\n",
    "# These rules will OVERWRITE the values for these specific articles,\n",
    "# even if they were set by the UOM rule above.\n",
    "\n",
    "# Now apply each rule one by one using .loc\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000002', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 10\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C230000165', 'Inbound Qty (UOM)'] = np.nan\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000093', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 5\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000023', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 5\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000226', 'Inbound Qty (UOM)'] = inbound_df['Qty Received']\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000021', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 10\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C230000534', 'Inbound Qty (UOM)'] = np.nan\n",
    "\n",
    "print(\"âœ… Step 2 Complete: 'Inbound Qty (UOM)' updated based on Article rules.\")\n",
    "\n",
    "# === FINAL CHECK: Print a sample to verify the column ===\n",
    "print(\"\\nðŸ” Final Check - Sample of relevant columns:\")\n",
    "print(inbound_df[['Article', 'UOM', 'Qty Received', 'Inbound Qty (UOM)']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8ec65e-9171-46f2-a5f0-574435256c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:19.313262Z",
     "iopub.status.busy": "2025-08-31T12:26:19.313262Z",
     "iopub.status.idle": "2025-08-31T12:26:19.323028Z",
     "shell.execute_reply": "2025-08-31T12:26:19.323028Z"
    },
    "papermill": {
     "duration": 0.047382,
     "end_time": "2025-08-31T12:26:19.324037",
     "exception": false,
     "start_time": "2025-08-31T12:26:19.276655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excel SUMIF formula converted!\n",
      "                  PO No  Inbound Qty (UOM) Inbound Qty > 8\n",
      "0   PO00058097-01082025               65.0            TRUE\n",
      "1   PO00058097-01082025               65.0            TRUE\n",
      "2   PO00058097-01082025               65.0            TRUE\n",
      "3   PO00058097-01082025               65.0            TRUE\n",
      "4   PO00058097-01082025               65.0            TRUE\n",
      "5   PO00058097-01082025               25.0            TRUE\n",
      "6           CGMU5171930               42.0            TRUE\n",
      "7           CGMU5171930               42.0            TRUE\n",
      "8           CGMU5171930               42.0            TRUE\n",
      "9           CGMU5171930               42.0            TRUE\n",
      "10          CGMU5171930               42.0            TRUE\n",
      "11          CGMU5171930               42.0            TRUE\n",
      "12          CGMU5171930               42.0            TRUE\n",
      "13          CGMU5171930               42.0            TRUE\n",
      "14          CGMU5171930               42.0            TRUE\n"
     ]
    }
   ],
   "source": [
    "# === CONVERT EXCEL FORMULA: SUMIF PO No > 8 ===\n",
    "\n",
    "# Calculate the total quantity for each PO No\n",
    "po_totals = inbound_df.groupby('PO No')['Inbound Qty (UOM)'].transform('sum')\n",
    "\n",
    "# Create the condition: if total for this PO No > 8, then True, else False\n",
    "inbound_df['Inbound Qty > 8'] = po_totals > 8\n",
    "\n",
    "# Convert boolean True/False to string \"TRUE\"/\"FALSE\" to match Excel\n",
    "inbound_df['Inbound Qty > 8'] = inbound_df['Inbound Qty > 8'].map({True: 'TRUE', False: 'FALSE'})\n",
    "\n",
    "print(\"âœ… Excel SUMIF formula converted!\")\n",
    "print(inbound_df[['PO No', 'Inbound Qty (UOM)', 'Inbound Qty > 8']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef7d055-b5e9-4b3f-a778-e3cca3cac723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:19.395871Z",
     "iopub.status.busy": "2025-08-31T12:26:19.391695Z",
     "iopub.status.idle": "2025-08-31T12:26:19.416947Z",
     "shell.execute_reply": "2025-08-31T12:26:19.416947Z"
    },
    "papermill": {
     "duration": 0.063188,
     "end_time": "2025-08-31T12:26:19.418995",
     "exception": false,
     "start_time": "2025-08-31T12:26:19.355807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excel date formula converted!\n",
      "    Receiving 1H / 2H(Storage)\n",
      "0  2025-08-01               1H\n",
      "1  2025-08-01               1H\n",
      "2  2025-08-01               1H\n",
      "3  2025-08-01               1H\n",
      "4  2025-08-01               1H\n",
      "5  2025-08-01               1H\n",
      "6  2025-08-01               1H\n",
      "7  2025-08-01               1H\n",
      "8  2025-08-01               1H\n",
      "9  2025-08-01               1H\n",
      "10 2025-08-01               1H\n",
      "11 2025-08-01               1H\n",
      "12 2025-08-01               1H\n",
      "13 2025-08-01               1H\n",
      "14 2025-08-01               1H\n"
     ]
    }
   ],
   "source": [
    "# === CONVERT EXCEL FORMULA: DATE TO 1H/2H CATEGORY ===\n",
    "\n",
    "# Convert the 'Receiving' column to datetime with DAY FIRST format (DD/MM/YYYY)\n",
    "inbound_df['Receiving'] = pd.to_datetime(inbound_df['Receiving'], dayfirst=True)\n",
    "\n",
    "# Extract the day of the month and apply the logic\n",
    "inbound_df['1H / 2H(Storage)'] = np.where(\n",
    "    inbound_df['Receiving'].dt.day <= 15,  # If day <= 15\n",
    "    '1H',                                  # Then \"1H\"\n",
    "    '2H'                                   # Else \"2H\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Excel date formula converted!\")\n",
    "print(inbound_df[['Receiving', '1H / 2H(Storage)']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f90c066-3fea-4194-974c-389f20482f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:19.472886Z",
     "iopub.status.busy": "2025-08-31T12:26:19.472886Z",
     "iopub.status.idle": "2025-08-31T12:26:21.210754Z",
     "shell.execute_reply": "2025-08-31T12:26:21.210754Z"
    },
    "papermill": {
     "duration": 1.768652,
     "end_time": "2025-08-31T12:26:21.216505",
     "exception": false,
     "start_time": "2025-08-31T12:26:19.447853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Product Detail Listing Loaded Successfully!\n",
      "Shape: (1437, 35) (rows, columns)\n",
      "\n",
      "First 5 rows:\n",
      "   Owner Code   Product Code  Description(2)                                                  Description(1)  Active  Alias1  UPC  Country Of Origin Product Status Pack Desc  Product Group Code  Product Group Description  Product Sub Group Code  Product Sub Group Description Product Category Product Class Storage Type Storage Type2 Allocation Type  Length (cm)  Breadth (cm)  Height (cm) Loose Whole  Whole Qty Issuing UOM Pallet  Pallet Qty  HS Code  Volume  Gross Weight Storage Category  Non Std WT  Min Days to Ship        Created Date\n",
      "0    10001201  ST-C200000037             NaN                ST-FB4103AS_BEEF TAIL IWP IW VAC HALAL BRAZIL GJ       1     NaN  NaN                NaN              N       NaN                 NaN                        NaN                     NaN                            NaN           FROZEN           NaN         FZ-H          FZ-H           FEFOS            0             0            0    KG    KG        1.0          KG    PLT      999999      NaN       0           0.0              NaN           0                 0 2025-02-28 14:24:28\n",
      "1    10001201  ST-C220000012             NaN   ST-CC0840AA_CHILLED CHIX 1.4-1.9 KG X 6NOS STEGGLES CAGE FREE       1     NaN  NaN                NaN              N       NaN                 NaN                        NaN                     NaN                            NaN           FROZEN           NaN         FZ-H          FZ-H           FEFOS            0             0            0    KG    KG        1.0          KG    PLT      999999      NaN       0           0.0              NaN           0                 0 2025-02-28 14:24:28\n",
      "2    10001201  ST-C220000014             NaN  ST-CC0842AA_CHILLED CHIX 1.4-1.65 KG X 6NOS STEGGLES CAGE FREE       1     NaN  NaN                NaN              N       NaN                 NaN                        NaN                     NaN                            NaN           FROZEN           NaN         FZ-H          FZ-H           FEFOS            0             0            0    KG    KG        1.0          KG    PLT      999999      NaN       0           0.0              NaN           0                 0 2025-02-28 14:24:28\n",
      "3    10001201  ST-C220000015             NaN  ST-CC0844AA_CHILLED CHIX 1.4-1.65KG X 6NOS LILYDALE RANGE FREE       1     NaN  NaN                NaN              N       NaN                 NaN                        NaN                     NaN                            NaN           FROZEN           NaN         FZ-H          FZ-H           FEFOS            0             0            0    KG    KG        1.0          KG    PLT      999999      NaN       0           0.0              NaN           0                 0 2025-02-28 14:24:28\n",
      "4    10001201  ST-C220000016             NaN             ST-CD1701XB_CHOOSY DICED GOUDA CHEESE 2 X 5KG HALAL       1     NaN  NaN                NaN              N       NaN                 NaN                        NaN                     NaN                            NaN           FROZEN           NaN         FZ-H          FZ-H           FEFOS            0             0            0   CTN   CTN        1.0         CTN    PLT      999999      NaN       0           0.0              NaN           0                 0 2025-02-28 14:24:28\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: LOAD PRODUCT DETAIL LISTING ===\n",
    "\n",
    "product_detail_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Product Detail Listing.xlsx\"\n",
    "\n",
    "# Read the product detail file into a DataFrame\n",
    "product_detail_df = pd.read_excel(product_detail_path)\n",
    "\n",
    "# Let's see what we're working with\n",
    "print(\"âœ… Product Detail Listing Loaded Successfully!\")\n",
    "print(f\"Shape: {product_detail_df.shape} (rows, columns)\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(product_detail_df.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5fc686e-dda1-4d2b-9eb1-8ad32a403e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:21.268383Z",
     "iopub.status.busy": "2025-08-31T12:26:21.268383Z",
     "iopub.status.idle": "2025-08-31T12:26:21.282886Z",
     "shell.execute_reply": "2025-08-31T12:26:21.282886Z"
    },
    "papermill": {
     "duration": 0.050657,
     "end_time": "2025-08-31T12:26:21.282886",
     "exception": false,
     "start_time": "2025-08-31T12:26:21.232229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mapping complete! 'Storage Type' column updated from Product Detail Listing.\n",
      "          Article Storage Type\n",
      "0   ST-C230000588       FROZEN\n",
      "1   ST-C230000588       FROZEN\n",
      "2   ST-C230000588       FROZEN\n",
      "3   ST-C230000588       FROZEN\n",
      "4   ST-C230000588       FROZEN\n",
      "5   ST-C230000588       FROZEN\n",
      "6   ST-C230000406       FROZEN\n",
      "7   ST-C230000406       FROZEN\n",
      "8   ST-C230000406       FROZEN\n",
      "9   ST-C230000406       FROZEN\n",
      "10  ST-C230000406       FROZEN\n",
      "11  ST-C230000406       FROZEN\n",
      "12  ST-C230000406       FROZEN\n",
      "13  ST-C230000406       FROZEN\n",
      "14  ST-C230000406       FROZEN\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: UPDATE EXISTING 'Storage Type' COLUMN ===\n",
    "\n",
    "# Create a mapping series from the product detail: Product Code -> Product Category\n",
    "storage_mapping = product_detail_df.set_index('Product Code')['Product Category']\n",
    "\n",
    "# Use the .map() function to update the 'Storage Type' column based on the 'Article' value\n",
    "inbound_df['Storage Type'] = inbound_df['Article'].map(storage_mapping)\n",
    "\n",
    "print(\"âœ… Mapping complete! 'Storage Type' column updated from Product Detail Listing.\")\n",
    "print(inbound_df[['Article', 'Storage Type']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec75ec13-40f8-45d5-9b37-d2e0c0a14e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:21.332194Z",
     "iopub.status.busy": "2025-08-31T12:26:21.332194Z",
     "iopub.status.idle": "2025-08-31T12:26:21.363064Z",
     "shell.execute_reply": "2025-08-31T12:26:21.361296Z"
    },
    "papermill": {
     "duration": 0.054655,
     "end_time": "2025-08-31T12:26:21.363064",
     "exception": false,
     "start_time": "2025-08-31T12:26:21.308409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conditional concatenation complete!\n",
      "   Storage Type   Receiving     Pallet ID    Location  Date_PalletID_Location\n",
      "0        FROZEN  2025-08-01  CFS20100684X  R040102207  2025-08-01CFS20100684X\n",
      "1        FROZEN  2025-08-01  CFS20104182U  R020402005  2025-08-01CFS20104182U\n",
      "2        FROZEN  2025-08-01  CFS20105611S  R020102309  2025-08-01CFS20105611S\n",
      "3        FROZEN  2025-08-01  CFS201068860  R030100510  2025-08-01CFS201068860\n",
      "4        FROZEN  2025-08-01  CFS20107275-  R010102408  2025-08-01CFS20107275-\n",
      "5        FROZEN  2025-08-01  CFS20107793+  R030402002  2025-08-01CFS20107793+\n",
      "6        FROZEN  2025-08-01  CFS20100116N  R010200105  2025-08-01CFS20100116N\n",
      "7        FROZEN  2025-08-01  CFS20100707T  R030301713  2025-08-01CFS20100707T\n",
      "8        FROZEN  2025-08-01  CFS20101151N  R010100513  2025-08-01CFS20101151N\n",
      "9        FROZEN  2025-08-01  CFS20101202K  R010302311  2025-08-01CFS20101202K\n",
      "10       FROZEN  2025-08-01  CFS20101652T  R010400206  2025-08-01CFS20101652T\n",
      "11       FROZEN  2025-08-01  CFS20101875-  R030301607  2025-08-01CFS20101875-\n",
      "12       FROZEN  2025-08-01  CFS20102333Q  R040101109  2025-08-01CFS20102333Q\n",
      "13       FROZEN  2025-08-01  CFS20102386Y  R030402504  2025-08-01CFS20102386Y\n",
      "14       FROZEN  2025-08-01  CFS20102599/  R040302010  2025-08-01CFS20102599/\n"
     ]
    }
   ],
   "source": [
    "# === CONDITIONAL CONCATENATION BASED ON STORAGE TYPE ===\n",
    "\n",
    "# First, ensure both columns are strings for concatenation\n",
    "inbound_df['Receiving'] = inbound_df['Receiving'].astype(str)\n",
    "inbound_df['Pallet ID'] = inbound_df['Pallet ID'].astype(str)\n",
    "inbound_df['Location'] = inbound_df['Location'].astype(str)\n",
    "\n",
    "# Apply different concatenation logic based on Storage Type\n",
    "inbound_df['Date_PalletID_Location'] = np.where(\n",
    "    inbound_df['Storage Type'] == 'FROZEN',\n",
    "    inbound_df['Receiving'] + inbound_df['Pallet ID'],      # For FROZEN: Receiving + Pallet ID\n",
    "    inbound_df['Receiving'] + inbound_df['Location']        # For others (CHILLED): Receiving + Location\n",
    ")\n",
    "\n",
    "print(\"âœ… Conditional concatenation complete!\")\n",
    "print(inbound_df[['Storage Type', 'Receiving', 'Pallet ID', 'Location', 'Date_PalletID_Location']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac94aa22-f079-4043-bcab-6bf4921cf138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:21.427071Z",
     "iopub.status.busy": "2025-08-31T12:26:21.424519Z",
     "iopub.status.idle": "2025-08-31T12:26:21.435169Z",
     "shell.execute_reply": "2025-08-31T12:26:21.435169Z"
    },
    "papermill": {
     "duration": 0.040418,
     "end_time": "2025-08-31T12:26:21.435169",
     "exception": false,
     "start_time": "2025-08-31T12:26:21.394751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clean up complete!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: CLEAN UP (OPTIONAL) ===\n",
    "\n",
    "# Drop the 'Product Code' column if you don't need it\n",
    "inbound_df.drop(columns=['Product Code'], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"âœ… Clean up complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "472ba610-9e39-4c88-8b3c-fbfe28a3646f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:21.481009Z",
     "iopub.status.busy": "2025-08-31T12:26:21.481009Z",
     "iopub.status.idle": "2025-08-31T12:26:33.422433Z",
     "shell.execute_reply": "2025-08-31T12:26:33.420416Z"
    },
    "papermill": {
     "duration": 11.960788,
     "end_time": "2025-08-31T12:26:33.425322",
     "exception": false,
     "start_time": "2025-08-31T12:26:21.464534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Using existing inbound_df for processing..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Checking expected columns in inbound_df:\n",
      "   âœ… '01_Inbound Type' exists\n",
      "   âœ… 'Inbound Qty (UOM)' exists\n",
      "   âœ… 'Inbound Qty > 8' exists\n",
      "   âœ… '1H / 2H(Storage)' exists\n",
      "   âœ… 'Unstuffing Type' exists\n",
      "   âœ… 'Unstuffing' exists\n",
      "   âœ… 'SFA Booking' exists\n",
      "   âœ… 'SFA Status' exists\n",
      "   âœ… 'Sorting' exists\n",
      "   âœ… 'Date_PalletID_Location' exists\n",
      "   âœ… 'Storage Type' exists\n",
      "   âœ… 'Remarks' exists\n",
      "ðŸ” Finding the correct sheet in SFA Status file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found target sheet: 'Master '\n",
      "ðŸ“– Loading SFA Status data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 'SFA Status' column with cleaned PO numbers\n",
      "   Loaded SFA status for 102 POs.\n",
      "âœ… Using 'PO No' as PO column\n",
      "âš™ï¸  Processing SFA Status lookup...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated SFA info for 440 records.\n",
      "   - Passed: 43\n",
      "   - Waived: 397\n",
      "âš™ï¸  Calculating sorting requirements...\n",
      "âœ… Using columns: PO='PO No', Article='Article', Expiry='Expiry Date', Remark='Remarks'\n",
      "âœ… Marked 48 POs for sorting\n",
      "ðŸ’¾ Saving processed data to Modified_Inbound_Data.xlsx...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data processing completed successfully!\n",
      "ðŸŽ¨ Applying formatting to headers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS! Formatted file saved to: C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\n",
      "âœ… Applied formatting to headers of key columns\n",
      "âœ… All data processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "# --- File Paths ---\n",
    "# CORRECTED PATH: Removed extra space and fixed folder structure\n",
    "sfa_status_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Products SFA Status 2025.xlsx\"\n",
    "modified_inbound_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\"\n",
    "\n",
    "# ==================== STEP 1: USE EXISTING inbound_df ====================\n",
    "\n",
    "print(\"ðŸ“– Using existing inbound_df for processing...\")\n",
    "\n",
    "# Verify we have the expected columns in inbound_df\n",
    "expected_columns = ['01_Inbound Type', 'Inbound Qty (UOM)', 'Inbound Qty > 8', '1H / 2H(Storage)', \n",
    "                   'Unstuffing Type', 'Unstuffing', 'SFA Booking', 'SFA Status', 'Sorting', \n",
    "                   'Date_PalletID_Location', 'Storage Type', 'Remarks']\n",
    "\n",
    "print(\"ðŸ“‹ Checking expected columns in inbound_df:\")\n",
    "for col in expected_columns:\n",
    "    if col in inbound_df.columns:\n",
    "        print(f\"   âœ… '{col}' exists\")\n",
    "    else:\n",
    "        print(f\"   âŒ '{col}' missing - adding now\")\n",
    "        inbound_df[col] = ''  # Add missing column with empty values\n",
    "\n",
    "# ==================== STEP 2: SFA STATUS LOOKUP ====================\n",
    "\n",
    "print(\"ðŸ” Finding the correct sheet in SFA Status file...\")\n",
    "try:\n",
    "    xl_sfa = pd.ExcelFile(sfa_status_path)\n",
    "    sfa_sheet_name = None\n",
    "\n",
    "    for sheet in xl_sfa.sheet_names:\n",
    "        df_check = pd.read_excel(sfa_status_path, sheet_name=sheet, nrows=5, header=None)\n",
    "        if not df_check.empty and len(df_check.columns) > 4 and df_check.iat[2, 4] == 'PO Number':\n",
    "            sfa_sheet_name = sheet\n",
    "            print(f\"   Found target sheet: '{sfa_sheet_name}'\")\n",
    "            break\n",
    "\n",
    "    if sfa_sheet_name is None:\n",
    "        raise ValueError(\"âŒ Could not find a sheet where cell E3 contains 'PO Number'. Please check the file.\")\n",
    "\n",
    "    print(\"ðŸ“– Loading SFA Status data...\")\n",
    "    df_sfa = pd.read_excel(sfa_status_path, sheet_name=sfa_sheet_name, header=2)\n",
    "\n",
    "    # Create mapping with cleaned PO numbers\n",
    "    if 'SFA Status' in df_sfa.columns:\n",
    "        df_sfa_clean = df_sfa[['PO Number', 'SFA Status']].dropna()\n",
    "        df_sfa_clean['PO Number Clean'] = df_sfa_clean['PO Number'].astype(str).str.strip().str.upper()\n",
    "        sfa_status_map = df_sfa_clean.set_index('PO Number Clean')['SFA Status']\n",
    "        print(\"âœ… Using 'SFA Status' column with cleaned PO numbers\")\n",
    "    else:\n",
    "        df_sfa_clean = df_sfa[['PO Number', df_sfa.columns[7]]].dropna()\n",
    "        df_sfa_clean['PO Number Clean'] = df_sfa_clean['PO Number'].astype(str).str.strip().str.upper()\n",
    "        sfa_status_map = df_sfa_clean.set_index('PO Number Clean').iloc[:, 0]\n",
    "        print(\"âœ… Using column H with cleaned PO numbers\")\n",
    "\n",
    "    print(f\"   Loaded SFA status for {len(sfa_status_map)} POs.\")\n",
    "\n",
    "    # Find the PO column in inbound data\n",
    "    po_column = None\n",
    "    for col in inbound_df.columns:\n",
    "        if 'po' in str(col).lower() or 'PO' in str(col):\n",
    "            po_column = col\n",
    "            break\n",
    "\n",
    "    if po_column is None:\n",
    "        raise ValueError(\"âŒ Could not find PO column in inbound data\")\n",
    "\n",
    "    print(f\"âœ… Using '{po_column}' as PO column\")\n",
    "\n",
    "    # Clean the PO numbers in the inbound data\n",
    "    inbound_df['PO No Clean'] = inbound_df[po_column].astype(str).str.strip().str.upper()\n",
    "\n",
    "    print(\"âš™ï¸  Processing SFA Status lookup...\")\n",
    "\n",
    "    def get_sfa_booking_status(po_number_clean):\n",
    "        if pd.notna(po_number_clean) and po_number_clean in sfa_status_map.index:\n",
    "            original_value = str(sfa_status_map[po_number_clean]).strip()\n",
    "            original_value_lower = original_value.lower()\n",
    "            \n",
    "            if 'waive' in original_value_lower:\n",
    "                return ('SFA', 'Waived')\n",
    "            elif 'pass' in original_value_lower:\n",
    "                return ('SFA', 'Passed')\n",
    "            else:\n",
    "                return (None, None)\n",
    "        else:\n",
    "            return (None, None)\n",
    "\n",
    "    # Apply SFA lookup to populate SFA Booking and SFA Status columns\n",
    "    update_count = 0\n",
    "    passed_count = 0\n",
    "    waived_count = 0\n",
    "\n",
    "    for index, row in inbound_df.iterrows():\n",
    "        po_number_clean = row['PO No Clean']\n",
    "        booking_val, status_val = get_sfa_booking_status(po_number_clean)\n",
    "        \n",
    "        if booking_val is not None:\n",
    "            inbound_df.at[index, 'SFA Booking'] = booking_val\n",
    "            inbound_df.at[index, 'SFA Status'] = status_val\n",
    "            update_count += 1\n",
    "            \n",
    "            if status_val == 'Passed':\n",
    "                passed_count += 1\n",
    "            elif status_val == 'Waived':\n",
    "                waived_count += 1\n",
    "\n",
    "    # Remove temporary column\n",
    "    inbound_df = inbound_df.drop('PO No Clean', axis=1)\n",
    "    print(f\"âœ… Updated SFA info for {update_count} records.\")\n",
    "    print(f\"   - Passed: {passed_count}\")\n",
    "    print(f\"   - Waived: {waived_count}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ SFA Status file not found. Skipping SFA lookup...\")\n",
    "    print(f\"   Expected path: {sfa_status_path}\")\n",
    "\n",
    "# ==================== STEP 3: SORTING LOGIC ====================\n",
    "\n",
    "print(\"âš™ï¸  Calculating sorting requirements...\")\n",
    "\n",
    "# Filter for Import rows only\n",
    "df_import = inbound_df[inbound_df[\"01_Inbound Type\"] == \"Import\"].copy()\n",
    "\n",
    "# Find required columns\n",
    "article_col = next((col for col in inbound_df.columns if 'article' in str(col).lower()), 'Article')\n",
    "expiry_col = next((col for col in inbound_df.columns if 'expiry' in str(col).lower()), 'Expiry Date')\n",
    "remark_col = next((col for col in inbound_df.columns if 'invt' in str(col).lower() or 'remark' in str(col).lower()), 'Invt Remark')\n",
    "\n",
    "print(f\"âœ… Using columns: PO='{po_column}', Article='{article_col}', Expiry='{expiry_col}', Remark='{remark_col}'\")\n",
    "\n",
    "# Build concatenated key and calculate unique combinations\n",
    "df_import[\"concat_key\"] = (\n",
    "    df_import[po_column].astype(str).fillna(\"\")\n",
    "    + \"_\" + df_import[article_col].astype(str).fillna(\"\")\n",
    "    + \"_\" + df_import[expiry_col].astype(str).fillna(\"\")\n",
    "    + \"_\" + df_import[remark_col].astype(str).fillna(\"\")\n",
    ")\n",
    "\n",
    "unique_counts = df_import.groupby(po_column)[\"concat_key\"].nunique()\n",
    "\n",
    "# Populate Sorting column\n",
    "inbound_df[\"Sorting\"] = \"\"\n",
    "inbound_df.loc[inbound_df[po_column].isin(unique_counts[unique_counts > 5].index), \"Sorting\"] = \"Sort\"\n",
    "\n",
    "sort_count = inbound_df[\"Sorting\"].value_counts().get(\"Sort\", 0)\n",
    "print(f\"âœ… Marked {sort_count} POs for sorting\")\n",
    "\n",
    "# ==================== STEP 4: SAVE PROCESSED DATA ====================\n",
    "\n",
    "print(\"ðŸ’¾ Saving processed data to Modified_Inbound_Data.xlsx...\")\n",
    "inbound_df.to_excel(modified_inbound_path, index=False, engine='openpyxl')\n",
    "print(\"âœ… Data processing completed successfully!\")\n",
    "\n",
    "# ==================== STEP 5: APPLY FORMATTING ====================\n",
    "\n",
    "print(\"ðŸŽ¨ Applying formatting to headers...\")\n",
    "columns_to_style = ['01_Inbound Type', 'Inbound Qty (UOM)', 'Inbound Qty > 8', '1H / 2H(Storage)',\n",
    "                   'Unstuffing Type', 'Unstuffing', 'SFA Booking', 'SFA Status', 'Sorting',\n",
    "                   'Date_PalletID_Location', 'Storage Type', 'Remarks']\n",
    "\n",
    "wb = load_workbook(modified_inbound_path)\n",
    "ws = wb.active\n",
    "\n",
    "light_yellow_fill = PatternFill(start_color='FFFFD6', end_color='FFFFD6', fill_type='solid')\n",
    "header_font = Font(name='Calibri', size=11, color='800000', bold=True)\n",
    "\n",
    "for col_idx, cell in enumerate(ws[1], 1):\n",
    "    if cell.value in columns_to_style:\n",
    "        cell.fill = light_yellow_fill\n",
    "        cell.font = header_font\n",
    "\n",
    "wb.save(modified_inbound_path)\n",
    "\n",
    "print(f\"âœ… SUCCESS! Formatted file saved to: {modified_inbound_path}\")\n",
    "print(f\"âœ… Applied formatting to headers of key columns\")\n",
    "print(f\"âœ… All data processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1f73d8-3c30-445c-9430-001978d6c5a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:33.477621Z",
     "iopub.status.busy": "2025-08-31T12:26:33.477621Z",
     "iopub.status.idle": "2025-08-31T12:26:40.970284Z",
     "shell.execute_reply": "2025-08-31T12:26:40.970284Z"
    },
    "papermill": {
     "duration": 7.517664,
     "end_time": "2025-08-31T12:26:40.970284",
     "exception": false,
     "start_time": "2025-08-31T12:26:33.452620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Loading existing Modified_Inbound_Data.xlsx...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  Converting Unstuffing columns to string type...\n",
      "ðŸ” Analyzing Import records for Unstuffing logic...\n",
      "ðŸ“Š Found 25 unique PO Numbers with Import type\n",
      "âœ… Unstuffing logic applied!\n",
      "   - 40 RF Container (Loose): 1057 records\n",
      "   - 20 RF Container (Loose): 0 records\n",
      "   - Total Import records processed: 1057\n",
      "ðŸ’¾ Saving updated data back to Modified_Inbound_Data.xlsx...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Unstuffing logic completed successfully!\n",
      "ðŸ“‹ Sample of updated records:\n",
      "      PO No 01_Inbound Type Date_PalletID_Location Unstuffing Type Unstuffing\n",
      "CGMU5171930          Import 2025-08-01CFS20100116N 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20100707T 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101151N 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101202K 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101652T 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101875- 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102333Q 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102386Y 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102599/ 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102820R 40 RF Container      Loose\n"
     ]
    }
   ],
   "source": [
    "# === NEW CELL: ADD UNSTUFFING LOGIC TO EXISTING MODIFIED_INBOUND_DATA ===\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "modified_inbound_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\"\n",
    "\n",
    "print(\"ðŸ“– Loading existing Modified_Inbound_Data.xlsx...\")\n",
    "# Load the already processed data\n",
    "df_modified = pd.read_excel(modified_inbound_path, engine='openpyxl')\n",
    "\n",
    "# FIX: Convert Unstuffing columns to string type first\n",
    "print(\"âš™ï¸  Converting Unstuffing columns to string type...\")\n",
    "df_modified['Unstuffing Type'] = df_modified['Unstuffing Type'].astype(str)\n",
    "df_modified['Unstuffing'] = df_modified['Unstuffing'].astype(str)\n",
    "\n",
    "# Replace 'nan' strings with empty string\n",
    "df_modified['Unstuffing Type'] = df_modified['Unstuffing Type'].replace('nan', '', regex=False)\n",
    "df_modified['Unstuffing'] = df_modified['Unstuffing'].replace('nan', '', regex=False)\n",
    "\n",
    "print(\"ðŸ” Analyzing Import records for Unstuffing logic...\")\n",
    "\n",
    "# Filter only Import records\n",
    "df_import = df_modified[df_modified[\"01_Inbound Type\"] == \"Import\"].copy()\n",
    "\n",
    "# Group by PO No and count unique Date_PalletID_Location for each PO\n",
    "unique_counts = df_import.groupby('PO No')['Date_PalletID_Location'].nunique()\n",
    "\n",
    "print(f\"ðŸ“Š Found {len(unique_counts)} unique PO Numbers with Import type\")\n",
    "\n",
    "# Apply Unstuffing logic based on unique count\n",
    "for po_number, unique_count in unique_counts.items():\n",
    "    if unique_count > 24:\n",
    "        # If > 24 unique locations, use 40 RF Container\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing Type'] = '40 RF Container'\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing'] = 'Loose'\n",
    "    else:\n",
    "        # If <= 24 unique locations, use 20 RF Container\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing Type'] = '20 RF Container'\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing'] = 'Loose'\n",
    "\n",
    "# Count how many POs got each Unstuffing Type\n",
    "forty_count = (df_modified['Unstuffing Type'] == '40 RF Container').sum()\n",
    "twenty_count = (df_modified['Unstuffing Type'] == '20 RF Container').sum()\n",
    "\n",
    "print(f\"âœ… Unstuffing logic applied!\")\n",
    "print(f\"   - 40 RF Container (Loose): {forty_count} records\")\n",
    "print(f\"   - 20 RF Container (Loose): {twenty_count} records\")\n",
    "print(f\"   - Total Import records processed: {len(df_import)}\")\n",
    "\n",
    "# Save back to the same file\n",
    "print(\"ðŸ’¾ Saving updated data back to Modified_Inbound_Data.xlsx...\")\n",
    "df_modified.to_excel(modified_inbound_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(\"âœ… Unstuffing logic completed successfully!\")\n",
    "print(\"ðŸ“‹ Sample of updated records:\")\n",
    "sample_df = df_modified[df_modified['01_Inbound Type'] == 'Import'][['PO No', '01_Inbound Type', 'Date_PalletID_Location', 'Unstuffing Type', 'Unstuffing']].head(10)\n",
    "print(sample_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cbc8d9b-5744-4082-9cfd-7a9f5944412b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T12:26:41.024620Z",
     "iopub.status.busy": "2025-08-31T12:26:41.021919Z",
     "iopub.status.idle": "2025-08-31T12:26:47.082005Z",
     "shell.execute_reply": "2025-08-31T12:26:47.082005Z"
    },
    "papermill": {
     "duration": 6.086712,
     "end_time": "2025-08-31T12:26:47.086531",
     "exception": false,
     "start_time": "2025-08-31T12:26:40.999819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¨ Applying formatting to headers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS! Formatted file saved to: C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\n",
      "âœ… Applied formatting to headers of key columns\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 5: APPLY FORMATTING ====================\n",
    "\n",
    "print(\"ðŸŽ¨ Applying formatting to headers...\")\n",
    "columns_to_style = ['01_Inbound Type', 'Inbound Qty (UOM)', 'Inbound Qty > 8', '1H / 2H(Storage)',\n",
    "                   'Unstuffing Type', 'Unstuffing', 'SFA Booking', 'SFA Status', 'Sorting',\n",
    "                   'Date_PalletID_Location', 'Storage Type', 'Remarks']\n",
    "\n",
    "wb = load_workbook(modified_inbound_path)\n",
    "ws = wb.active\n",
    "\n",
    "light_yellow_fill = PatternFill(start_color='FFFFD6', end_color='FFFFD6', fill_type='solid')\n",
    "header_font = Font(name='Calibri', size=11, color='800000', bold=True)\n",
    "\n",
    "for col_idx, cell in enumerate(ws[1], 1):\n",
    "    if cell.value in columns_to_style:\n",
    "        cell.fill = light_yellow_fill\n",
    "        cell.font = header_font\n",
    "\n",
    "wb.save(modified_inbound_path)\n",
    "\n",
    "print(f\"âœ… SUCCESS! Formatted file saved to: {modified_inbound_path}\")\n",
    "print(f\"âœ… Applied formatting to headers of key columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a0b4d-63a6-47d5-806f-cedee294cf8c",
   "metadata": {
    "papermill": {
     "duration": 0.025612,
     "end_time": "2025-08-31T12:26:47.127997",
     "exception": false,
     "start_time": "2025-08-31T12:26:47.102385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46.271533,
   "end_time": "2025-08-31T12:26:48.388821",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\Users\\erictr\\CF Inbound Billing Method.ipynb",
   "output_path": "C:\\Users\\erictr\\CF Inbound Billing Method.ipynb",
   "parameters": {},
   "start_time": "2025-08-31T12:26:02.117288",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
