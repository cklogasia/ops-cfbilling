{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfada7b0-ea66-414c-b7a7-83ea3fe2ba9a",
   "metadata": {
    "papermill": {
     "duration": 5.182304,
     "end_time": "2025-09-10T02:58:56.074453",
     "exception": false,
     "start_time": "2025-09-10T02:58:50.892149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === SUPPRESS OPENPYXL WARNINGS ===\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\n",
    "\n",
    "# === STEP 1: LOAD THE NEW OUTBOUND DATA ===\n",
    "# === INITIAL SETUP: RUN THIS CELL FIRST ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to your new Inbound file\n",
    "inbound_file_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Inbound Report Data.xlsx\" # <<< CHANGE THIS TO YOUR ACTUAL FILE PATH\n",
    "\n",
    "# Read the new file into a DataFrame called 'inbound_df'\n",
    "inbound_df = pd.read_excel(inbound_file_path)\n",
    "\n",
    "# Let's see what we're working with\n",
    "print(\"âœ… Inbound Data Loaded Successfully!\")\n",
    "print(f\"Shape: {inbound_df.shape} (rows, columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94753348-c225-4102-b45c-985bf4c57045",
   "metadata": {
    "papermill": {
     "duration": 0.043896,
     "end_time": "2025-09-10T02:58:56.141907",
     "exception": false,
     "start_time": "2025-09-10T02:58:56.098011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add new columns with default values\n",
    "inbound_df['01_Inbound Type'] = ''\n",
    "inbound_df['Inbound Qty (UOM)'] = 0\n",
    "inbound_df['Inbound Qty > 8'] = ''\n",
    "inbound_df['1H / 2H(Storage)'] = ''\n",
    "inbound_df['Unstuffing Type'] = ''\n",
    "inbound_df['Unstuffing'] = ''\n",
    "inbound_df['SFA Booking'] = ''\n",
    "inbound_df['SFA Status'] = ''\n",
    "inbound_df['Sorting'] = ''\n",
    "inbound_df['Date_PalletID_Location'] = ''\n",
    "inbound_df['Storage Type'] = ''\n",
    "inbound_df['Remarks'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8680f-96a2-40f0-ba64-1676c73a5651",
   "metadata": {
    "papermill": {
     "duration": 0.062842,
     "end_time": "2025-09-10T02:58:56.224996",
     "exception": false,
     "start_time": "2025-09-10T02:58:56.162154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === APPLY EXCEL FORMULA: REFERENCE D2, RESULT IN AC2 ===\n",
    "\n",
    "# First, create a clean series by trimming whitespace from the D2 column (the reference)\n",
    "clean_series = inbound_df['PO No'].str.strip()  # TRIM values from D2 column\n",
    "\n",
    "# Normalize: trim spaces + lowercase for case-insensitive checks\n",
    "clean_series_norm = clean_series.str.strip().str.lower()\n",
    "\n",
    "conditions = [\n",
    "    clean_series_norm.str.startswith('po'),\n",
    "    clean_series_norm.str.startswith('to'),\n",
    "    clean_series_norm.str.startswith('sro'),\n",
    "    clean_series_norm.str.startswith('for '),\n",
    "    clean_series_norm.str.startswith('ret'),\n",
    "    clean_series_norm.str.startswith('order'),\n",
    "    clean_series_norm == ''\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    'Receiving',\n",
    "    'Plant Transfer', \n",
    "    'Return',\n",
    "    'Return',\n",
    "    'Return',\n",
    "    'Return',\n",
    "    '-'  # This handles the empty string case\n",
    "]\n",
    "\n",
    "# Apply the conditions and put the result in AC2 column\n",
    "inbound_df['01_Inbound Type'] = np.select(conditions, choices, default='Import')\n",
    "\n",
    "print(\"âœ… Excel formula applied! Reference: PO No, Result: 01_Inbound Type\")\n",
    "print(inbound_df[['PO No', '01_Inbound Type']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51a53e-1e74-4716-aa5e-00709423ba81",
   "metadata": {
    "papermill": {
     "duration": 0.062231,
     "end_time": "2025-09-10T02:58:56.311503",
     "exception": false,
     "start_time": "2025-09-10T02:58:56.249272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === PART 1: Update 'Inbound Qty (UOM)' for UOM == 'CTN' ===\n",
    "# This sets the value for rows where UOM is 'CTN'\n",
    "inbound_df.loc[inbound_df['UOM'] == 'CTN', 'Inbound Qty (UOM)'] = inbound_df.loc[inbound_df['UOM'] == 'CTN', 'Qty Received']\n",
    "print(\"âœ… Step 1 Complete: 'Inbound Qty (UOM)' updated for UOM = 'CTN'\")\n",
    "\n",
    "# === PART 2: OVERRIDE 'Inbound Qty (UOM)' for specific Articles ===\n",
    "# These rules will OVERWRITE the values for these specific articles,\n",
    "# even if they were set by the UOM rule above.\n",
    "\n",
    "# Now apply each rule one by one using .loc\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000002', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 10\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C230000165', 'Inbound Qty (UOM)'] = np.nan\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000093', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 5\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000023', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 5\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000226', 'Inbound Qty (UOM)'] = inbound_df['Qty Received']\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C730000021', 'Inbound Qty (UOM)'] = inbound_df['Qty Received'] / 10\n",
    "inbound_df.loc[inbound_df['Article'] == 'ST-C230000534', 'Inbound Qty (UOM)'] = np.nan\n",
    "\n",
    "print(\"âœ… Step 2 Complete: 'Inbound Qty (UOM)' updated based on Article rules.\")\n",
    "\n",
    "# === FINAL CHECK: Print a sample to verify the column ===\n",
    "print(\"\\nðŸ” Final Check - Sample of relevant columns:\")\n",
    "print(inbound_df[['Article', 'UOM', 'Qty Received', 'Inbound Qty (UOM)']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ec65e-9171-46f2-a5f0-574435256c65",
   "metadata": {
    "papermill": {
     "duration": 0.061212,
     "end_time": "2025-09-10T02:58:56.405118",
     "exception": false,
     "start_time": "2025-09-10T02:58:56.343906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === CONVERT EXCEL FORMULA: SUMIF PO No > 8 ===\n",
    "\n",
    "# Calculate the total quantity for each PO No\n",
    "po_totals = inbound_df.groupby('PO No')['Inbound Qty (UOM)'].transform('sum')\n",
    "\n",
    "# Create the condition: if total for this PO No > 8, then True, else False\n",
    "inbound_df['Inbound Qty > 8'] = po_totals > 8\n",
    "\n",
    "# Convert boolean True/False to string \"TRUE\"/\"FALSE\" to match Excel\n",
    "inbound_df['Inbound Qty > 8'] = inbound_df['Inbound Qty > 8'].map({True: 'TRUE', False: 'FALSE'})\n",
    "\n",
    "print(\"âœ… Excel SUMIF formula converted!\")\n",
    "print(inbound_df[['PO No', 'Inbound Qty (UOM)', 'Inbound Qty > 8']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7d055-b5e9-4b3f-a778-e3cca3cac723",
   "metadata": {
    "papermill": {
     "duration": 0.055359,
     "end_time": "2025-09-10T02:58:56.490685",
     "exception": false,
     "start_time": "2025-09-10T02:58:56.435326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Receiving 1H / 2H(Storage)\n",
      "0  2025-08-01               1H\n",
      "1  2025-08-01               1H\n",
      "2  2025-08-01               1H\n",
      "3  2025-08-01               1H\n",
      "4  2025-08-01               1H\n",
      "5  2025-08-01               1H\n",
      "6  2025-08-01               1H\n",
      "7  2025-08-01               1H\n",
      "8  2025-08-01               1H\n",
      "9  2025-08-01               1H\n",
      "10 2025-08-01               1H\n",
      "11 2025-08-01               1H\n",
      "12 2025-08-01               1H\n",
      "13 2025-08-01               1H\n",
      "14 2025-08-01               1H\n"
     ]
    }
   ],
   "source": [
    "# === CONVERT EXCEL FORMULA: DATE TO 1H/2H CATEGORY ===\n",
    "\n",
    "# Convert the 'Receiving' column to datetime with DAY FIRST format (DD/MM/YYYY)\n",
    "inbound_df['Receiving'] = pd.to_datetime(inbound_df['Receiving'], dayfirst=True)\n",
    "\n",
    "# Extract the day of the month and apply the logic\n",
    "inbound_df['1H / 2H(Storage)'] = np.where(\n",
    "    inbound_df['Receiving'].dt.day <= 15,  # If day <= 15\n",
    "    '1H',                                  # Then \"1H\"\n",
    "    '2H'                                   # Else \"2H\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Excel date formula converted!\")\n",
    "print(inbound_df[['Receiving', '1H / 2H(Storage)']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90c066-3fea-4194-974c-389f20482f51",
   "metadata": {
    "papermill": {
     "duration": 1.880417,
     "end_time": "2025-09-10T02:58:58.395510",
     "exception": false,
     "start_time": "2025-09-10T02:58:56.515093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === STEP 1: LOAD PRODUCT DETAIL LISTING ===\n",
    "\n",
    "product_detail_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Product Detail Listing.xlsx\"\n",
    "\n",
    "# Read the product detail file into a DataFrame\n",
    "product_detail_df = pd.read_excel(product_detail_path)\n",
    "\n",
    "# Let's see what we're working with\n",
    "print(\"âœ… Product Detail Listing Loaded Successfully!\")\n",
    "print(f\"Shape: {product_detail_df.shape} (rows, columns)\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(product_detail_df.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc686e-dda1-4d2b-9eb1-8ad32a403e1b",
   "metadata": {
    "papermill": {
     "duration": 0.047359,
     "end_time": "2025-09-10T02:58:58.470642",
     "exception": false,
     "start_time": "2025-09-10T02:58:58.423283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === STEP 2: UPDATE EXISTING 'Storage Type' COLUMN ===\n",
    "\n",
    "# Normalize both keys to uppercase for case-insensitive matching\n",
    "product_detail_df['Product Code Clean'] = product_detail_df['Product Code'].astype(str).str.strip().str.upper()\n",
    "inbound_df['Article Clean'] = inbound_df['Article'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Create a mapping series from the cleaned product detail: Product Code -> Product Category\n",
    "storage_mapping = product_detail_df.set_index('Product Code Clean')['Product Category']\n",
    "\n",
    "# Map using the cleaned Article, but store result in original 'Storage Type' column\n",
    "inbound_df['Storage Type'] = inbound_df['Article Clean'].map(storage_mapping)\n",
    "\n",
    "# Drop the temporary helper column if you donâ€™t need it later\n",
    "inbound_df = inbound_df.drop(columns=['Article Clean'])\n",
    "\n",
    "print(\"âœ… Mapping complete! 'Storage Type' column updated from Product Detail Listing (case-insensitive).\")\n",
    "print(inbound_df[['Article', 'Storage Type']].head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75ec13-40f8-45d5-9b37-d2e0c0a14e33",
   "metadata": {
    "papermill": {
     "duration": 0.052611,
     "end_time": "2025-09-10T02:58:58.549907",
     "exception": false,
     "start_time": "2025-09-10T02:58:58.497296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === CONDITIONAL CONCATENATION BASED ON STORAGE TYPE ===\n",
    "\n",
    "# First, ensure both columns are strings for concatenation\n",
    "inbound_df['Receiving'] = inbound_df['Receiving'].astype(str)\n",
    "inbound_df['Pallet ID'] = inbound_df['Pallet ID'].astype(str)\n",
    "inbound_df['Location'] = inbound_df['Location'].astype(str)\n",
    "\n",
    "# Apply different concatenation logic based on Storage Type\n",
    "inbound_df['Date_PalletID_Location'] = np.where(\n",
    "    inbound_df['Storage Type'] == 'FROZEN',\n",
    "    inbound_df['Receiving'] + inbound_df['Pallet ID'],      # For FROZEN: Receiving + Pallet ID\n",
    "    inbound_df['Receiving'] + inbound_df['Location']        # For others (CHILLED): Receiving + Location\n",
    ")\n",
    "\n",
    "print(\"âœ… Conditional concatenation complete!\")\n",
    "print(inbound_df[['Storage Type', 'Receiving', 'Pallet ID', 'Location', 'Date_PalletID_Location']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94aa22-f079-4043-bcab-6bf4921cf138",
   "metadata": {
    "papermill": {
     "duration": 0.038403,
     "end_time": "2025-09-10T02:58:58.614054",
     "exception": false,
     "start_time": "2025-09-10T02:58:58.575651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === STEP 3: CLEAN UP (OPTIONAL) ===\n",
    "\n",
    "# Drop the 'Product Code' column if you don't need it\n",
    "inbound_df.drop(columns=['Product Code'], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"âœ… Clean up complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ba610-9e39-4c88-8b3c-fbfe28a3646f",
   "metadata": {
    "papermill": {
     "duration": 11.210707,
     "end_time": "2025-09-10T02:59:09.846761",
     "exception": false,
     "start_time": "2025-09-10T02:58:58.636054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found target sheet: 'Master '\n",
      "ðŸ“– Loading SFA Status data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 'SFA Status' column with cleaned PO numbers\n",
      "   Loaded SFA status for 102 POs.\n",
      "âœ… Using 'PO No' as PO column\n",
      "âš™ï¸  Processing SFA Status lookup...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated SFA info for 440 records.\n",
      "   - Passed: 43\n",
      "   - Waived: 397\n",
      "âš™ï¸  Calculating sorting requirements...\n",
      "âœ… Using columns: PO='PO No', Article='Article', Expiry='Expiry Date', Remark='Remarks'\n",
      "âœ… Marked 48 POs for sorting\n",
      "ðŸ’¾ Saving processed data to Modified_Inbound_Data.xlsx...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data processing completed successfully!\n",
      "ðŸŽ¨ Applying formatting to headers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS! Formatted file saved to: C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\n",
      "âœ… Applied formatting to headers of key columns\n",
      "âœ… All data processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "# --- File Paths ---\n",
    "# CORRECTED PATH: Removed extra space and fixed folder structure\n",
    "sfa_status_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Products SFA Status 2025.xlsx\"\n",
    "modified_inbound_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\"\n",
    "\n",
    "# ==================== STEP 1: USE EXISTING inbound_df ====================\n",
    "\n",
    "print(\"ðŸ“– Using existing inbound_df for processing...\")\n",
    "\n",
    "# Verify we have the expected columns in inbound_df\n",
    "expected_columns = ['01_Inbound Type', 'Inbound Qty (UOM)', 'Inbound Qty > 8', '1H / 2H(Storage)', \n",
    "                   'Unstuffing Type', 'Unstuffing', 'SFA Booking', 'SFA Status', 'Sorting', \n",
    "                   'Date_PalletID_Location', 'Storage Type', 'Remarks']\n",
    "\n",
    "print(\"ðŸ“‹ Checking expected columns in inbound_df:\")\n",
    "for col in expected_columns:\n",
    "    if col in inbound_df.columns:\n",
    "        print(f\"   âœ… '{col}' exists\")\n",
    "    else:\n",
    "        print(f\"   âŒ '{col}' missing - adding now\")\n",
    "        inbound_df[col] = ''  # Add missing column with empty values\n",
    "\n",
    "# ==================== STEP 2: SFA STATUS LOOKUP ====================\n",
    "\n",
    "print(\"ðŸ” Finding the correct sheet in SFA Status file...\")\n",
    "try:\n",
    "    xl_sfa = pd.ExcelFile(sfa_status_path)\n",
    "    sfa_sheet_name = None\n",
    "\n",
    "    for sheet in xl_sfa.sheet_names:\n",
    "        df_check = pd.read_excel(sfa_status_path, sheet_name=sheet, nrows=5, header=None)\n",
    "        if not df_check.empty and len(df_check.columns) > 4 and df_check.iat[2, 4] == 'PO Number':\n",
    "            sfa_sheet_name = sheet\n",
    "            print(f\"   Found target sheet: '{sfa_sheet_name}'\")\n",
    "            break\n",
    "\n",
    "    if sfa_sheet_name is None:\n",
    "        raise ValueError(\"âŒ Could not find a sheet where cell E3 contains 'PO Number'. Please check the file.\")\n",
    "\n",
    "    print(\"ðŸ“– Loading SFA Status data...\")\n",
    "    df_sfa = pd.read_excel(sfa_status_path, sheet_name=sfa_sheet_name, header=2)\n",
    "\n",
    "    # Create mapping with cleaned PO numbers\n",
    "    if 'SFA Status' in df_sfa.columns:\n",
    "        df_sfa_clean = df_sfa[['PO Number', 'SFA Status']].dropna()\n",
    "        df_sfa_clean['PO Number Clean'] = df_sfa_clean['PO Number'].astype(str).str.strip().str.upper()\n",
    "        sfa_status_map = df_sfa_clean.set_index('PO Number Clean')['SFA Status']\n",
    "        print(\"âœ… Using 'SFA Status' column with cleaned PO numbers\")\n",
    "    else:\n",
    "        df_sfa_clean = df_sfa[['PO Number', df_sfa.columns[7]]].dropna()\n",
    "        df_sfa_clean['PO Number Clean'] = df_sfa_clean['PO Number'].astype(str).str.strip().str.upper()\n",
    "        sfa_status_map = df_sfa_clean.set_index('PO Number Clean').iloc[:, 0]\n",
    "        print(\"âœ… Using column H with cleaned PO numbers\")\n",
    "\n",
    "    print(f\"   Loaded SFA status for {len(sfa_status_map)} POs.\")\n",
    "\n",
    "    # Find the PO column in inbound data\n",
    "    po_column = None\n",
    "    for col in inbound_df.columns:\n",
    "        if 'po' in str(col).lower() or 'PO' in str(col):\n",
    "            po_column = col\n",
    "            break\n",
    "\n",
    "    if po_column is None:\n",
    "        raise ValueError(\"âŒ Could not find PO column in inbound data\")\n",
    "\n",
    "    print(f\"âœ… Using '{po_column}' as PO column\")\n",
    "\n",
    "    # Clean the PO numbers in the inbound data\n",
    "    inbound_df['PO No Clean'] = inbound_df[po_column].astype(str).str.strip().str.upper()\n",
    "\n",
    "    print(\"âš™ï¸  Processing SFA Status lookup...\")\n",
    "\n",
    "    def get_sfa_booking_status(po_number_clean):\n",
    "        if pd.notna(po_number_clean) and po_number_clean in sfa_status_map.index:\n",
    "            original_value = str(sfa_status_map[po_number_clean]).strip()\n",
    "            original_value_lower = original_value.lower()\n",
    "            \n",
    "            if 'waive' in original_value_lower:\n",
    "                return ('SFA', 'Waived')\n",
    "            elif 'pass' in original_value_lower:\n",
    "                return ('SFA', 'Passed')\n",
    "            else:\n",
    "                return (None, None)\n",
    "        else:\n",
    "            return (None, None)\n",
    "\n",
    "    # Apply SFA lookup to populate SFA Booking and SFA Status columns\n",
    "    update_count = 0\n",
    "    passed_count = 0\n",
    "    waived_count = 0\n",
    "\n",
    "    for index, row in inbound_df.iterrows():\n",
    "        po_number_clean = row['PO No Clean']\n",
    "        booking_val, status_val = get_sfa_booking_status(po_number_clean)\n",
    "        \n",
    "        if booking_val is not None:\n",
    "            inbound_df.at[index, 'SFA Booking'] = booking_val\n",
    "            inbound_df.at[index, 'SFA Status'] = status_val\n",
    "            update_count += 1\n",
    "            \n",
    "            if status_val == 'Passed':\n",
    "                passed_count += 1\n",
    "            elif status_val == 'Waived':\n",
    "                waived_count += 1\n",
    "\n",
    "    # Remove temporary column\n",
    "    inbound_df = inbound_df.drop('PO No Clean', axis=1)\n",
    "    print(f\"âœ… Updated SFA info for {update_count} records.\")\n",
    "    print(f\"   - Passed: {passed_count}\")\n",
    "    print(f\"   - Waived: {waived_count}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ SFA Status file not found. Skipping SFA lookup...\")\n",
    "    print(f\"   Expected path: {sfa_status_path}\")\n",
    "\n",
    "# ==================== STEP 3: SORTING LOGIC ====================\n",
    "\n",
    "print(\"âš™ï¸  Calculating sorting requirements...\")\n",
    "\n",
    "# Filter for Import rows only\n",
    "df_import = inbound_df[inbound_df[\"01_Inbound Type\"] == \"Import\"].copy()\n",
    "\n",
    "# Find required columns\n",
    "article_col = next((col for col in inbound_df.columns if 'article' in str(col).lower()), 'Article')\n",
    "expiry_col = next((col for col in inbound_df.columns if 'expiry' in str(col).lower()), 'Expiry Date')\n",
    "remark_col = next((col for col in inbound_df.columns if 'invt' in str(col).lower() or 'remark' in str(col).lower()), 'Invt Remark')\n",
    "\n",
    "print(f\"âœ… Using columns: PO='{po_column}', Article='{article_col}', Expiry='{expiry_col}', Remark='{remark_col}'\")\n",
    "\n",
    "# Build concatenated key and calculate unique combinations\n",
    "df_import[\"concat_key\"] = (\n",
    "    df_import[po_column].astype(str).fillna(\"\")\n",
    "    + \"_\" + df_import[article_col].astype(str).fillna(\"\")\n",
    "    + \"_\" + df_import[expiry_col].astype(str).fillna(\"\")\n",
    "    + \"_\" + df_import[remark_col].astype(str).fillna(\"\")\n",
    ")\n",
    "\n",
    "unique_counts = df_import.groupby(po_column)[\"concat_key\"].nunique()\n",
    "\n",
    "# Populate Sorting column\n",
    "inbound_df[\"Sorting\"] = \"\"\n",
    "inbound_df.loc[inbound_df[po_column].isin(unique_counts[unique_counts > 5].index), \"Sorting\"] = \"Sort\"\n",
    "\n",
    "sort_count = inbound_df[\"Sorting\"].value_counts().get(\"Sort\", 0)\n",
    "print(f\"âœ… Marked {sort_count} POs for sorting\")\n",
    "\n",
    "# ==================== STEP 4: SAVE PROCESSED DATA ====================\n",
    "\n",
    "print(\"ðŸ’¾ Saving processed data to Modified_Inbound_Data.xlsx...\")\n",
    "inbound_df.to_excel(modified_inbound_path, index=False, engine='openpyxl')\n",
    "print(\"âœ… Data processing completed successfully!\")\n",
    "\n",
    "# ==================== STEP 5: APPLY FORMATTING ====================\n",
    "\n",
    "print(\"ðŸŽ¨ Applying formatting to headers...\")\n",
    "columns_to_style = ['01_Inbound Type', 'Inbound Qty (UOM)', 'Inbound Qty > 8', '1H / 2H(Storage)',\n",
    "                   'Unstuffing Type', 'Unstuffing', 'SFA Booking', 'SFA Status', 'Sorting',\n",
    "                   'Date_PalletID_Location', 'Storage Type', 'Remarks']\n",
    "\n",
    "wb = load_workbook(modified_inbound_path)\n",
    "ws = wb.active\n",
    "\n",
    "light_yellow_fill = PatternFill(start_color='FFFFD6', end_color='FFFFD6', fill_type='solid')\n",
    "header_font = Font(name='Calibri', size=11, color='800000', bold=True)\n",
    "\n",
    "for col_idx, cell in enumerate(ws[1], 1):\n",
    "    if cell.value in columns_to_style:\n",
    "        cell.fill = light_yellow_fill\n",
    "        cell.font = header_font\n",
    "\n",
    "wb.save(modified_inbound_path)\n",
    "\n",
    "print(f\"âœ… SUCCESS! Formatted file saved to: {modified_inbound_path}\")\n",
    "print(f\"âœ… Applied formatting to headers of key columns\")\n",
    "print(f\"âœ… All data processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f73d8-3c30-445c-9430-001978d6c5a1",
   "metadata": {
    "papermill": {
     "duration": 7.282977,
     "end_time": "2025-09-10T02:59:17.159284",
     "exception": false,
     "start_time": "2025-09-10T02:59:09.876307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  Converting Unstuffing columns to string type...\n",
      "ðŸ” Analyzing Import records for Unstuffing logic...\n",
      "ðŸ“Š Found 25 unique PO Numbers with Import type\n",
      "âœ… Unstuffing logic applied!\n",
      "   - 40 RF Container (Loose): 1057 records\n",
      "   - 20 RF Container (Loose): 0 records\n",
      "   - Total Import records processed: 1057\n",
      "ðŸ’¾ Saving updated data back to Modified_Inbound_Data.xlsx...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Unstuffing logic completed successfully!\n",
      "ðŸ“‹ Sample of updated records:\n",
      "      PO No 01_Inbound Type Date_PalletID_Location Unstuffing Type Unstuffing\n",
      "CGMU5171930          Import 2025-08-01CFS20100116N 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20100707T 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101151N 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101202K 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101652T 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20101875- 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102333Q 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102386Y 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102599/ 40 RF Container      Loose\n",
      "CGMU5171930          Import 2025-08-01CFS20102820R 40 RF Container      Loose\n"
     ]
    }
   ],
   "source": [
    "# === NEW CELL: ADD UNSTUFFING LOGIC TO EXISTING MODIFIED_INBOUND_DATA ===\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "modified_inbound_path = r\"C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\"\n",
    "\n",
    "print(\"ðŸ“– Loading existing Modified_Inbound_Data.xlsx...\")\n",
    "# Load the already processed data\n",
    "df_modified = pd.read_excel(modified_inbound_path, engine='openpyxl')\n",
    "\n",
    "# FIX: Convert Unstuffing columns to string type first\n",
    "print(\"âš™ï¸  Converting Unstuffing columns to string type...\")\n",
    "df_modified['Unstuffing Type'] = df_modified['Unstuffing Type'].astype(str)\n",
    "df_modified['Unstuffing'] = df_modified['Unstuffing'].astype(str)\n",
    "\n",
    "# Replace 'nan' strings with empty string\n",
    "df_modified['Unstuffing Type'] = df_modified['Unstuffing Type'].replace('nan', '', regex=False)\n",
    "df_modified['Unstuffing'] = df_modified['Unstuffing'].replace('nan', '', regex=False)\n",
    "\n",
    "print(\"ðŸ” Analyzing Import records for Unstuffing logic...\")\n",
    "\n",
    "# Filter only Import records\n",
    "df_import = df_modified[df_modified[\"01_Inbound Type\"] == \"Import\"].copy()\n",
    "\n",
    "# Group by PO No and count unique Date_PalletID_Location for each PO\n",
    "unique_counts = df_import.groupby('PO No')['Date_PalletID_Location'].nunique()\n",
    "\n",
    "print(f\"ðŸ“Š Found {len(unique_counts)} unique PO Numbers with Import type\")\n",
    "\n",
    "# Apply Unstuffing logic based on unique count\n",
    "for po_number, unique_count in unique_counts.items():\n",
    "    if unique_count > 24:\n",
    "        # If > 24 unique locations, use 40 RF Container\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing Type'] = '40 RF Container'\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing'] = 'Loose'\n",
    "    else:\n",
    "        # If <= 24 unique locations, use 20 RF Container\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing Type'] = '20 RF Container'\n",
    "        df_modified.loc[(df_modified['PO No'] == po_number) & (df_modified['01_Inbound Type'] == 'Import'), \n",
    "                       'Unstuffing'] = 'Loose'\n",
    "\n",
    "# Count how many POs got each Unstuffing Type\n",
    "forty_count = (df_modified['Unstuffing Type'] == '40 RF Container').sum()\n",
    "twenty_count = (df_modified['Unstuffing Type'] == '20 RF Container').sum()\n",
    "\n",
    "print(f\"âœ… Unstuffing logic applied!\")\n",
    "print(f\"   - 40 RF Container (Loose): {forty_count} records\")\n",
    "print(f\"   - 20 RF Container (Loose): {twenty_count} records\")\n",
    "print(f\"   - Total Import records processed: {len(df_import)}\")\n",
    "\n",
    "# Save back to the same file\n",
    "print(\"ðŸ’¾ Saving updated data back to Modified_Inbound_Data.xlsx...\")\n",
    "df_modified.to_excel(modified_inbound_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(\"âœ… Unstuffing logic completed successfully!\")\n",
    "print(\"ðŸ“‹ Sample of updated records:\")\n",
    "sample_df = df_modified[df_modified['01_Inbound Type'] == 'Import'][['PO No', '01_Inbound Type', 'Date_PalletID_Location', 'Unstuffing Type', 'Unstuffing']].head(10)\n",
    "print(sample_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc8d9b-5744-4082-9cfd-7a9f5944412b",
   "metadata": {
    "papermill": {
     "duration": 6.548498,
     "end_time": "2025-09-10T02:59:23.725929",
     "exception": false,
     "start_time": "2025-09-10T02:59:17.177431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS! Formatted file saved to: C:\\Users\\erictr\\Documents\\Python Billing Project\\Inbound Data\\Result\\Modified_Inbound_Data.xlsx\n",
      "âœ… Applied formatting to headers of key columns\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 5: APPLY FORMATTING ====================\n",
    "\n",
    "print(\"ðŸŽ¨ Applying formatting to headers...\")\n",
    "columns_to_style = ['01_Inbound Type', 'Inbound Qty (UOM)', 'Inbound Qty > 8', '1H / 2H(Storage)',\n",
    "                   'Unstuffing Type', 'Unstuffing', 'SFA Booking', 'SFA Status', 'Sorting',\n",
    "                   'Date_PalletID_Location', 'Storage Type', 'Remarks']\n",
    "\n",
    "wb = load_workbook(modified_inbound_path)\n",
    "ws = wb.active\n",
    "\n",
    "light_yellow_fill = PatternFill(start_color='FFFFD6', end_color='FFFFD6', fill_type='solid')\n",
    "header_font = Font(name='Calibri', size=11, color='800000', bold=True)\n",
    "\n",
    "for col_idx, cell in enumerate(ws[1], 1):\n",
    "    if cell.value in columns_to_style:\n",
    "        cell.fill = light_yellow_fill\n",
    "        cell.font = header_font\n",
    "\n",
    "wb.save(modified_inbound_path)\n",
    "\n",
    "print(f\"âœ… SUCCESS! Formatted file saved to: {modified_inbound_path}\")\n",
    "print(f\"âœ… Applied formatting to headers of key columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a0b4d-63a6-47d5-806f-cedee294cf8c",
   "metadata": {
    "papermill": {
     "duration": 0.017512,
     "end_time": "2025-09-10T02:59:23.758020",
     "exception": false,
     "start_time": "2025-09-10T02:59:23.740508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40.062993,
   "end_time": "2025-09-10T02:59:24.727477",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\Users\\erictr\\CF Inbound Billing Method.ipynb",
   "output_path": "C:\\Users\\erictr\\CF Inbound Billing Method.ipynb",
   "parameters": {},
   "start_time": "2025-09-10T02:58:44.664484",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
